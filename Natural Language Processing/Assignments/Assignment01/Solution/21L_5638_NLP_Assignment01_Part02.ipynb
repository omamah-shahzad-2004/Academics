{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZFZvkz8u_xfP"
   },
   "outputs": [],
   "source": [
    "##\n",
    "import os.path\n",
    "import sys\n",
    "import random\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SwFGqsQKAEm8"
   },
   "outputs": [],
   "source": [
    "#----------------------------------------\n",
    "#  Data input\n",
    "#----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "obnWxWYaAj1q"
   },
   "outputs": [],
   "source": [
    "# Read a text file into a corpus (list of sentences (which in turn are lists of words))\n",
    "# (taken from nested section of HW0)\n",
    "def readFileToCorpus(f):\n",
    "    \"\"\" Reads in the text file f which contains one sentence per line.\n",
    "    \"\"\"\n",
    "    if os.path.isfile(f):\n",
    "        file = open(f, \"r\") # open the input file in read-only mode\n",
    "        i = 0 # this is just a counter to keep track of the sentence numbers\n",
    "        corpus = [] # this will become a list of sentences\n",
    "        print(\"Reading file \", f)\n",
    "        for line in file:\n",
    "            i += 1\n",
    "            sentence = line.split() # split the line into a list of words\n",
    "            #append this list as an element to the list of sentences\n",
    "            corpus.append(sentence)\n",
    "            if i % 1000 == 0:\n",
    "    \t#print a status message: str(i) turns int i into a string\n",
    "    \t#so we can concatenate it\n",
    "                sys.stderr.write(\"Reading sentence \" + str(i) + \"\\n\")\n",
    "        #endif\n",
    "    #endfor\n",
    "        return corpus\n",
    "    else:\n",
    "    #ideally we would throw an exception here, but this will suffice\n",
    "        print(\"Error: corpus file \", f, \" does not exist\")\n",
    "        sys.exit() # exit the script\n",
    "    #endif\n",
    "#enddef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QpEQYJyZhXQB"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "UNK = \"UNK\"     # Unknown word token\n",
    "start = \"<s>\"   # Start-of-sentence token\n",
    "end = \"</s>\"    # End-of-sentence-token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "klZIPY8yBQjh"
   },
   "outputs": [],
   "source": [
    "# Preprocess the corpus\n",
    "def preprocess(corpus):\n",
    "    #find all the rare words\n",
    "    freqDict = defaultdict(int)\n",
    "    for sen in corpus:\n",
    "\t    for word in sen:\n",
    "\t       freqDict[word] += 1\n",
    "\t#endfor\n",
    "    #endfor\n",
    "\n",
    "    #replace rare words with unk\n",
    "    for sen in corpus:\n",
    "        for i in range(0, len(sen)):\n",
    "            word = sen[i]\n",
    "            if freqDict[word] < 2:\n",
    "\n",
    "                sen[i] = UNK\n",
    "\t    #endif\n",
    "\t#endfor\n",
    "    #endfor\n",
    "\n",
    "    #bookend the sentences with start and end tokens\n",
    "    for sen in corpus:\n",
    "        sen.insert(0, start)\n",
    "        sen.append(end)\n",
    "    #endfor\n",
    "\n",
    "    return corpus\n",
    "#enddef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cFSafis4BoyH"
   },
   "outputs": [],
   "source": [
    "def preprocessTest(vocab, corpus):\n",
    "    #replace test words that were unseen in the training with unk\n",
    "    for sen in corpus:\n",
    "        for i in range(0, len(sen)):\n",
    "            word = sen[i]\n",
    "            if word not in vocab:\n",
    "                sen[i] = UNK\n",
    "\t    #endif\n",
    "\t#endfor\n",
    "    #endfor\n",
    "\n",
    "    #bookend the sentences with start and end tokens\n",
    "    for sen in corpus:\n",
    "        sen.insert(0, start)\n",
    "        sen.append(end)\n",
    "    #endfor\n",
    "\n",
    "    return corpus\n",
    "#enddef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Fz1bEKsTB-cl"
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------\n",
    "# Language models and data structures\n",
    "#--------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fAXxKQw7CXtQ"
   },
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "    def __init__(self, corpus):\n",
    "        pass\n",
    "    #enddef\n",
    "\n",
    "    def generateSentence(self):\n",
    "        pass\n",
    "    #emddef\n",
    "\n",
    "    def getSentenceProbability(self, sen):\n",
    "        pass\n",
    "    #enddef\n",
    "\n",
    "    def getCorpusPerplexity(self, corpus):\n",
    "        pass\n",
    "    #enddef\n",
    "\n",
    "    def generateSentencesToFile(self, numberOfSentences, filename):\n",
    "        filePointer = open(filename, 'w+')\n",
    "        for i in range(0,numberOfSentences):\n",
    "            sen = self.generateSentence()\n",
    "            prob = self.getSentenceProbability(sen)\n",
    "\n",
    "            stringGenerated = str(prob) + \" \" + \" \".join(sen)\n",
    "            print(stringGenerated, end=\"\\n\", file=filePointer)\n",
    "\n",
    "\t#endfor\n",
    "    #enddef\n",
    "#endclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "izlW-FD7opyD"
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "lOauDt6QCfjS"
   },
   "outputs": [],
   "source": [
    "# Unigram language model\n",
    "class UnigramModel(LanguageModel):\n",
    "    def __init__(self, corpus):\n",
    "        self.counts = defaultdict(float)\n",
    "        self.total = 0.0\n",
    "        self.train(corpus)\n",
    "    #endddef\n",
    "\n",
    "    # Add observed counts from corpus to the distribution\n",
    "    def train(self, corpus):\n",
    "        for sen in corpus:\n",
    "            for word in sen:\n",
    "                if word == '<s>':\n",
    "                    continue\n",
    "                self.counts[word] += 1.0\n",
    "                self.total += 1.0\n",
    "            #endfor\n",
    "        #endfor\n",
    "    #enddef\n",
    "\n",
    "    # Returns the probability of word in the distribution\n",
    "    def prob(self, word):\n",
    "        return self.counts[word]/self.total\n",
    "    #enddef\n",
    "\n",
    "    # Generate a single random word according to the distribution\n",
    "    def draw(self):\n",
    "        rand = random.random()\n",
    "        for word in self.counts.keys():\n",
    "            rand -= self.prob(word)\n",
    "            if rand <= 0.0:\n",
    "                return word\n",
    "\t    #endif\n",
    "\t#endfor\n",
    "    #enddef\n",
    "\n",
    "    def generateSentence(self):\n",
    "        sentence = []\n",
    "        sentence.append('<s>')\n",
    "        while True:\n",
    "            word = self.draw()\n",
    "            if word == '</s>':\n",
    "                sentence.append(word)\n",
    "                break\n",
    "            sentence.append(word)\n",
    "        return sentence\n",
    "\n",
    "    def getSentenceProbability(self, sen):\n",
    "        senProbability = 1.0\n",
    "        for word in sen:\n",
    "            senProbability *= self.prob(word)\n",
    "        return senProbability\n",
    "\n",
    "    def getCorpusPerplexity(self, corpus):\n",
    "        corpusProbability = 1.0\n",
    "        word_count = 0.0\n",
    "        for sen in corpus:\n",
    "            for word in sen:\n",
    "                corpusProbability *= self.prob(word)\n",
    "                word_count += 1\n",
    "        normalized_corpusProbability = corpusProbability / word_count\n",
    "        corpusPerplexity = -normalized_corpusProbability\n",
    "        return corpusPerplexity\n",
    "    #enddef\n",
    "#endclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "VIwqvPH1Cj5s"
   },
   "outputs": [],
   "source": [
    "#Smoothed unigram language model\n",
    "class SmoothedUnigramModel(LanguageModel):\n",
    "    def __init__(self, corpus):\n",
    "        self.counts = defaultdict(float)\n",
    "        self.total = 0.0\n",
    "        self.train(corpus)\n",
    "        self.vocab_size = 0.0\n",
    "    #endddef\n",
    "\n",
    "    # Add observed counts from corpus to the distribution\n",
    "    def train(self, corpus):\n",
    "        for sen in corpus:\n",
    "            for word in sen:\n",
    "                if word == '<s>':\n",
    "                    continue\n",
    "                self.counts[word] += 1.0\n",
    "                self.total += 1.0\n",
    "            #endfor\n",
    "        #endfor\n",
    "        self.vocab_size = len(self.counts)\n",
    "    #enddef\n",
    "\n",
    "    # Returns the probability of word in the distribution\n",
    "    def prob(self, word):\n",
    "        return (self.counts[word]+1.0)/(self.total+self.vocab_size)\n",
    "    #enddef\n",
    "\n",
    "    # Generate a single random word according to the distribution\n",
    "    def draw(self):\n",
    "        rand = random.random()\n",
    "        for word in self.counts.keys():\n",
    "            rand -= self.prob(word)\n",
    "            if rand <= 0.0:\n",
    "                return word\n",
    "\t    #endif\n",
    "\t#endfor\n",
    "    #enddef\n",
    "\n",
    "    def generateSentence(self):\n",
    "        sentence = []\n",
    "        sentence.append('<s>')\n",
    "        while True:\n",
    "            word = self.draw()\n",
    "            if word == '</s>':\n",
    "                sentence.append(word)\n",
    "                break\n",
    "            sentence.append(word)\n",
    "        return sentence\n",
    "\n",
    "    def getSentenceProbability(self, sen):\n",
    "        senProbability = 0.0\n",
    "        for word in sen:\n",
    "            senProbability += math.log(self.prob(word))\n",
    "        senProbability = math.exp(senProbability)\n",
    "        return senProbability\n",
    "\n",
    "    def getCorpusPerplexity(self, corpus):\n",
    "        corpusProbability = 0.0\n",
    "        word_count = 0.0\n",
    "        for sen in corpus:\n",
    "            for word in sen:\n",
    "                corpusProbability += math.log(self.prob(word))\n",
    "                word_count += 1\n",
    "        normalized_corpusProbability = corpusProbability / word_count\n",
    "        corpusPerplexity = math.exp(-normalized_corpusProbability)\n",
    "        return corpusPerplexity\n",
    "    #enddef\n",
    "#endclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BSsNO8bZ7jcQ"
   },
   "outputs": [],
   "source": [
    "# Unsmoothed bigram language model\n",
    "class BigramModel(LanguageModel):\n",
    "    def __init__(self, corpus):\n",
    "        self.unigram_counts = defaultdict(float)\n",
    "        self.bigram_counts = defaultdict(lambda: defaultdict(float))\n",
    "        self.total = 0.0\n",
    "        self.train(corpus)\n",
    "    #endddef\n",
    "\n",
    "    # Add observed counts from corpus to the distribution\n",
    "    def train(self, corpus):\n",
    "        for sen in corpus:\n",
    "            prev_word = '<s>'\n",
    "            for word in sen:\n",
    "                if word == '<s>':\n",
    "                    continue\n",
    "                self.unigram_counts[prev_word] += 1.0\n",
    "                self.bigram_counts[prev_word][word] += 1.0\n",
    "                self.total += 1.0\n",
    "                prev_word = word\n",
    "            #endfor\n",
    "            self.unigram_counts[prev_word] += 1.0\n",
    "            self.total += 1.0\n",
    "        #endfor\n",
    "        self.unigram_counts['<s>'] += len(corpus)\n",
    "        self.total += len(corpus)\n",
    "    #enddef\n",
    "\n",
    "    # Returns the probability of word in the distribution\n",
    "    def prob(self, prev_word, word):\n",
    "        return self.bigram_counts[prev_word][word]/self.unigram_counts[prev_word]\n",
    "    #enddef\n",
    "\n",
    "    # Generate a single random word according to the distribution\n",
    "    def draw(self, prev_word):\n",
    "        rand = random.random()\n",
    "        for word in self.bigram_counts[prev_word].keys():\n",
    "            rand -= self.prob(prev_word, word)\n",
    "            if rand <= 0.0:\n",
    "                return word\n",
    "\t    #endif\n",
    "\t#endfor\n",
    "        return word\n",
    "    #enddef\n",
    "\n",
    "    def generateSentence(self):\n",
    "        sentence = []\n",
    "        prev_word = '<s>'\n",
    "        sentence.append(prev_word)\n",
    "        while True:\n",
    "            word = self.draw(prev_word)\n",
    "            if word == '</s>':\n",
    "                sentence.append(word)\n",
    "                break\n",
    "            sentence.append(word)\n",
    "            prev_word = word\n",
    "        return sentence\n",
    "\n",
    "    def getSentenceProbability(self, sen):\n",
    "        senProbability = 1.0\n",
    "        prev_word = '<s>'\n",
    "        for word in sen:\n",
    "            senProbability *= self.prob(prev_word, word)\n",
    "            prev_word = word\n",
    "        return senProbability\n",
    "\n",
    "    def getCorpusPerplexity(self, corpus):\n",
    "        corpusProbability = 1.0\n",
    "        word_count = 0.0\n",
    "        for sen in corpus:\n",
    "            prev_word = '<s>'\n",
    "            for word in sen:\n",
    "                corpusProbability *= self.prob(prev_word, word)\n",
    "                word_count += 1\n",
    "                prev_word = word\n",
    "        normalized_corpusProbability = corpusProbability / word_count\n",
    "        corpusPerplexity = -normalized_corpusProbability\n",
    "        return corpusPerplexity\n",
    "    #enddef\n",
    "#endclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "L84JqnAUC0DE"
   },
   "outputs": [],
   "source": [
    "# Smoothed bigram language model\n",
    "class SmoothedBigramModelKN(LanguageModel):\n",
    "    def __init__(self, corpus):\n",
    "        self.unigram_counts = defaultdict(float)\n",
    "        self.bigram_counts = defaultdict(lambda: defaultdict(float))\n",
    "        self.total = 0.0\n",
    "        self.train(corpus)\n",
    "    #endddef\n",
    "\n",
    "    # Add observed counts from corpus to the distribution\n",
    "    def train(self, corpus):\n",
    "        for sen in corpus:\n",
    "            prev_word = '<s>'\n",
    "            for word in sen:\n",
    "                if word == '<s>':\n",
    "                    continue\n",
    "                self.unigram_counts[prev_word] += 1.0\n",
    "                self.bigram_counts[prev_word][word] += 1.0\n",
    "                self.total += 1.0\n",
    "                prev_word = word\n",
    "            #endfor\n",
    "            self.unigram_counts[prev_word] += 1.0\n",
    "            self.total += 1.0\n",
    "        #endfor\n",
    "        self.unigram_counts['<s>'] += len(corpus)\n",
    "        self.total += len(corpus)\n",
    "    #enddef\n",
    "\n",
    "    # Returns the probability of word in the distribution\n",
    "    def prob(self, prev_word, word):\n",
    "        lambda1 = 0.5\n",
    "        lambda2 = 0.5\n",
    "        unigram_probability = self.unigram_counts[word]/self.total\n",
    "        bigram_probability = self.bigram_counts[prev_word][word]/self.unigram_counts[prev_word]\n",
    "        return ((lambda1 * unigram_probability) + (lambda2 * bigram_probability))\n",
    "    #enddef\n",
    "\n",
    "    # Generate a single random word according to the distribution\n",
    "    def draw(self, prev_word):\n",
    "        rand = random.random()\n",
    "        for word in self.bigram_counts[prev_word].keys():\n",
    "            rand -= self.prob(prev_word, word)\n",
    "            if rand <= 0.0:\n",
    "                return word\n",
    "\t    #endif\n",
    "\t#endfor\n",
    "        return word\n",
    "    #enddef\n",
    "\n",
    "    def generateSentence(self):\n",
    "        sentence = []\n",
    "        prev_word = '<s>'\n",
    "        sentence.append(prev_word)\n",
    "        while True:\n",
    "            word = self.draw(prev_word)\n",
    "            if word == '</s>':\n",
    "                sentence.append(word)\n",
    "                break\n",
    "            sentence.append(word)\n",
    "            prev_word = word\n",
    "        return sentence\n",
    "\n",
    "    def getSentenceProbability(self, sen):\n",
    "        senProbability = 1.0\n",
    "        prev_word = '<s>'\n",
    "        for word in sen:\n",
    "            senProbability *= self.prob(prev_word, word)\n",
    "            prev_word = word\n",
    "        return senProbability\n",
    "\n",
    "    def getCorpusPerplexity(self, corpus):\n",
    "        corpusProbability = 0.0\n",
    "        word_count = 0.0\n",
    "        for sen in corpus:\n",
    "            prev_word = '<s>'\n",
    "            for word in sen:\n",
    "                corpusProbability += math.log(self.prob(prev_word, word))\n",
    "                word_count += 1\n",
    "                prev_word = word\n",
    "        normalized_corpusProbability = corpusProbability / word_count\n",
    "        corpusPerplexity = math.exp(-normalized_corpusProbability)\n",
    "        return corpusPerplexity\n",
    "    #enddef\n",
    "#endclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jfmPGQndC6bm",
    "outputId": "2b7df851-2136-4be5-8526-5a0bd3c14f8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file  train.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading sentence 1000\n",
      "Reading sentence 2000\n",
      "Reading sentence 3000\n",
      "Reading sentence 4000\n",
      "Reading sentence 5000\n",
      "Reading sentence 6000\n",
      "Reading sentence 7000\n",
      "Reading sentence 8000\n",
      "Reading sentence 9000\n",
      "Reading sentence 10000\n",
      "Reading sentence 11000\n",
      "Reading sentence 12000\n",
      "Reading sentence 13000\n",
      "Reading sentence 14000\n",
      "Reading sentence 15000\n",
      "Reading sentence 16000\n",
      "Reading sentence 17000\n",
      "Reading sentence 18000\n",
      "Reading sentence 19000\n",
      "Reading sentence 20000\n",
      "Reading sentence 21000\n",
      "Reading sentence 22000\n",
      "Reading sentence 23000\n",
      "Reading sentence 24000\n",
      "Reading sentence 25000\n",
      "Reading sentence 26000\n",
      "Reading sentence 27000\n",
      "Reading sentence 28000\n",
      "Reading sentence 29000\n",
      "Reading sentence 30000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file  pos_test.txt\n",
      "Reading file  neg_test.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading sentence 1000\n",
      "Reading sentence 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Perplexity (Negative Test Corpus): -0.0\n",
      "Unigram Perplexity (Positive Test Corpus): -0.0\n",
      "Smoothed Unigram Perplexity (Negative Test Corpus): 793.4530597408765\n",
      "Smoothed Unigram Perplexity (Positive Test Corpus): 804.4353683904808\n",
      "Bigram Perplexity (Negative Test Corpus): -0.0\n",
      "Bigram Perplexity (Positive Test Corpus): -0.0\n",
      "Smoothed Bigram Perplexity (Negative Test Corpus): 240.89746020937125\n",
      "Smoothed Bigram Perplexity (Positive Test Corpus): 234.33789802250433\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------\n",
    "# The main routine\n",
    "#-------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    #read your corpora\n",
    "    trainCorpus = readFileToCorpus('train.txt')\n",
    "    trainCorpus = preprocess(trainCorpus)\n",
    "\n",
    "    posTestCorpus = readFileToCorpus('pos_test.txt')\n",
    "    negTestCorpus = readFileToCorpus('neg_test.txt')\n",
    "\n",
    "    vocab = set(word for sent in trainCorpus for word in sent)\n",
    "\n",
    "\n",
    "    posTestCorpus = preprocessTest(vocab, posTestCorpus)\n",
    "    negTestCorpus = preprocessTest(vocab, negTestCorpus)\n",
    "\n",
    "    unigramModel = UnigramModel(trainCorpus)\n",
    "    smoothed_unigramModel = SmoothedUnigramModel(trainCorpus)\n",
    "    bigramModel = BigramModel(trainCorpus)\n",
    "    smoothed_bigramModel = SmoothedBigramModelKN(trainCorpus)\n",
    "\n",
    "    unigramModel.generateSentencesToFile(20, 'unigram_output.txt')\n",
    "    smoothed_unigramModel.generateSentencesToFile(20, 'smooth_unigram_output.txt')\n",
    "    bigramModel.generateSentencesToFile(20, 'bigram_output.txt')\n",
    "    smoothed_bigramModel.generateSentencesToFile(20, 'smooth_bigram_kn_output.txt')\n",
    "\n",
    "    unigram_perplexity_4negTest = unigramModel.getCorpusPerplexity(negTestCorpus)\n",
    "    unigram_perplexity_4posTest = unigramModel.getCorpusPerplexity(posTestCorpus)\n",
    "    smoothed_unigram_perplexity_4negTest = smoothed_unigramModel.getCorpusPerplexity(negTestCorpus)\n",
    "    smoothed_unigram_perplexity_4posTest = smoothed_unigramModel.getCorpusPerplexity(posTestCorpus)\n",
    "\n",
    "    bigram_perplexity_4negTest = bigramModel.getCorpusPerplexity(negTestCorpus)\n",
    "    bigram_perplexity_4posTest = bigramModel.getCorpusPerplexity(posTestCorpus)\n",
    "    smoothed_bigram_perplexity_4negTest = smoothed_bigramModel.getCorpusPerplexity(negTestCorpus)\n",
    "    smoothed_bigram_perplexity_4posTest = smoothed_bigramModel.getCorpusPerplexity(posTestCorpus)\n",
    "\n",
    "    print('Unigram Perplexity (Negative Test Corpus):', unigram_perplexity_4negTest)\n",
    "    print('Unigram Perplexity (Positive Test Corpus):', unigram_perplexity_4posTest)\n",
    "    print('Smoothed Unigram Perplexity (Negative Test Corpus):', smoothed_unigram_perplexity_4negTest)\n",
    "    print('Smoothed Unigram Perplexity (Positive Test Corpus):', smoothed_unigram_perplexity_4posTest)\n",
    "\n",
    "    print('Bigram Perplexity (Negative Test Corpus):', bigram_perplexity_4negTest)\n",
    "    print('Bigram Perplexity (Positive Test Corpus):', bigram_perplexity_4posTest)\n",
    "    print('Smoothed Bigram Perplexity (Negative Test Corpus):', smoothed_bigram_perplexity_4negTest)\n",
    "    print('Smoothed Bigram Perplexity (Positive Test Corpus):', smoothed_bigram_perplexity_4posTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Question#01)\n",
    "#In Unigram model, the length of the generated sentences is controlled by the probability of occurrence of individual words in a\n",
    "#training corpus. Rather in Bigram model, generation is controlled by trasition probabilities between consecutive words. This\n",
    "#means that the next word in the sentence is chosen based on the probability of it following the previous word, which can result\n",
    "#in more structured sentences as compared to unigram model.\n",
    "\n",
    "#(Question#02)\n",
    "#Yes, the models do assign drastically different probabilities to the different sets of sentences. This is because each model \n",
    "#captures different aspects of language structure. Unigram model considers each word independently, which may result in less\n",
    "#realistic sentences. On the other hand, Bigram model take into account the relationships between adjacent words, resulting in \n",
    "#more realistic sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Question#03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramModel.generateSentencesToFile(5, 'bigram_output_(2).txt')\n",
    "smoothed_bigramModel.generateSentencesToFile(5, 'smooth_bigram_kn_output_(2).txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In my opinion, the Smoothed Bigram model produces better sentences because, it incorporates Linear \n",
    "#Interpolation smoothing to handle unseen word pairs, resulting in more realistic sentences as compared to the basic bigram \n",
    "#model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Question#04)\n",
    "#Perplexity values for each model:-\n",
    "#Unigram Perplexity (Negative Test Corpus): -0.0\n",
    "#Unigram Perplexity (Positive Test Corpus): -0.0\n",
    "#Smoothed Unigram Perplexity (Negative Test Corpus): 793.4530597408765\n",
    "#Smoothed Unigram Perplexity (Positive Test Corpus): 804.4353683904808\n",
    "#Bigram Perplexity (Negative Test Corpus): -0.0\n",
    "#Bigram Perplexity (Positive Test Corpus): -0.0\n",
    "#Smoothed Bigram Perplexity (Negative Test Corpus): 240.89746020937125\n",
    "#Smoothed Bigram Perplexity (Positive Test Corpus): 234.33789802250433\n",
    "\n",
    "#The Smoothed Unigram model has the highest perplexity among the four models. This is because, the model fails to capture the\n",
    "#underlying language patterns in the test corpus, leading to higher perplexity as compared to the other models."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
